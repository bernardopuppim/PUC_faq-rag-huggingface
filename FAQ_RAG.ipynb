{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab89696e",
   "metadata": {},
   "source": [
    "\n",
    "# FAQ RAG — Notebook Único (FAISS + MiniLM + FLAN‑T5)\n",
    "\n",
    "1) **Ingestão** de `.txt` locais  \n",
    "2) **Embeddings** com `all-MiniLM-L6-v2`  \n",
    "3) **Indexação** com FAISS  \n",
    "4) **Recuperação** top‑k  \n",
    "5) **Geração** com `flan-t5-base` (100% local) \n",
    "6) **UI opcional** com Gradio dentro do notebook  \n",
    "\n",
    "> Requisitos: `faiss-cpu`, `sentence-transformers`, `transformers`, `accelerate`, `torch`, `pandas`, `gradio` (opcional).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7575f81",
   "metadata": {},
   "source": [
    "# OpenAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98e28c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import openai\n",
    "\n",
    "# Ler config.ini\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "# Pegar a chave\n",
    "openai.api_key = config[\"openai\"][\"api_key\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd086fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 chunks criados (seção‑cientes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berna\\AppData\\Local\\Temp\\ipykernel_14456\\3207146501.py:53: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re, configparser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ========== 1) Ler chave do config.ini ==========\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "OPENAI_API_KEY = config[\"openai\"][\"api_key\"]\n",
    "\n",
    "# ====== 2) Split seção-ciente + sub-split por tamanho ======\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "ARQUIVO = Path(\"guia_equipamentos_snowboard.txt\")\n",
    "texto = ARQUIVO.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# 2.1 primeiro quebrar por seção \"###\"\n",
    "secoes = [s.strip() for s in re.split(r\"\\n(?=### )\", texto) if s.strip()]\n",
    "\n",
    "# 2.2 sub-split para seções longas (preserva o título dentro de cada pedaço)\n",
    "subsplitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,      \n",
    "    chunk_overlap= 50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for sec in secoes:\n",
    "    # se a seção é curta, fica inteira; se for longa, quebramos com overlap\n",
    "    if len(sec) <= 700:\n",
    "        docs.append(Document(page_content=sec, metadata={\"source\": ARQUIVO.name}))\n",
    "    else:\n",
    "        parts = subsplitter.split_text(sec)\n",
    "        for p in parts:\n",
    "            # garante que o título \"### ...\" permaneça no pedaço\n",
    "            if not p.lstrip().startswith(\"###\"):\n",
    "                # pega o cabeçalho da seção original\n",
    "                header = sec.splitlines()[0]\n",
    "                p = header + \"\\n\" + p\n",
    "            docs.append(Document(page_content=p.strip(), metadata={\"source\": ARQUIVO.name}))\n",
    "\n",
    "print(f\"{len(docs)} chunks criados (seção‑cientes)\")\n",
    "\n",
    "# ====== 3) Embeddings + FAISS (modelo multilíngue melhor p/ PT-BR) ======\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "vectorstore = FAISS.from_documents(docs, emb)\n",
    "\n",
    "# ====== 4) LLM OpenAI (gpt-4o-mini) ======\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-4o-mini\", temperature=0.3)\n",
    "\n",
    "# ====== 5) Prompt enxuto ======\n",
    "from langchain.prompts import PromptTemplate\n",
    "PROMPT = PromptTemplate.from_template(\"\"\"\\\n",
    "Você é um assistente em português.\n",
    "Responda à pergunta de forma **curta (2–4 frases)**, clara e natural, usando **SOMENTE** o contexto.\n",
    "Não copie frases do contexto; reescreva com suas palavras.\n",
    "Se a resposta não estiver no contexto, diga: \"Não encontrei essa informação nos documentos.\"\n",
    "\n",
    "[PERGUNTA]\n",
    "{question}\n",
    "\n",
    "[CONTEXTO]\n",
    "{context}\n",
    "\n",
    "[RESPOSTA]\n",
    "\"\"\")\n",
    "\n",
    "# ====== 6) RetrievalQA (MMR melhora foco) ======\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",           # em vez de pure similarity\n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 13, \"lambda_mult\": 0.5}\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaee7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== 7) Teste ======\n",
    "pergunta = \"Como escolher o tamanho ideial para a prancha de snowboard?\"\n",
    "resp = qa.invoke({\"query\": pergunta})\n",
    "\n",
    "print(\"\\nPergunta:\", pergunta)\n",
    "print(\"\\nResposta:\\n\", resp[\"result\"])\n",
    "print(\"\\nFontes:\")\n",
    "for d in resp[\"source_documents\"]:\n",
    "    print(\"-\", d.metadata.get(\"source\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a72f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berna\\AppData\\Local\\Temp\\ipykernel_14456\\2327325990.py:80: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  inp.on_submit(_submit)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636e8c67d36d488a96039b3d18910095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='', layout=Layout(width='100%'), placeholder='Digite sua pergunta e t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CHAT (ipywidgets) com dedupe + quebra de linha ---\n",
    "from ipywidgets import Text, Textarea, Button, Output, VBox, HBox, IntSlider, Checkbox\n",
    "from IPython.display import display\n",
    "from textwrap import shorten\n",
    "\n",
    "try:\n",
    "    qa  # noqa\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Objeto `qa` não encontrado. Crie o RetrievalQA antes do widget.\")\n",
    "\n",
    "inp = Text(placeholder=\"Digite sua pergunta e tecle Enter\", layout={'width': '100%'})\n",
    "send_btn = Button(description=\"Enviar\")\n",
    "clear_btn = Button(description=\"Limpar\", button_style=\"warning\")\n",
    "k_slider = IntSlider(value=1, min=1, max=5, step=1, description='k:', continuous_update=False)\n",
    "show_sources = Checkbox(value=True, description='Mostrar fontes')\n",
    "\n",
    "# Transcript como Textarea (quebra linha + scroll)\n",
    "log = Textarea(\n",
    "    value=\"RAG Chat (ipywidgets) pronto! Ajuste k se quiser, pergunte algo como: 'Quais os tipos de shape de prancha de snowboard?'\\n\",\n",
    "    layout={'width': '100%', 'height': '360px'},\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "# aplica k inicial\n",
    "try:\n",
    "    qa.retriever.search_kwargs.update({\"k\": k_slider.value})\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def _set_k(change=None):\n",
    "    try:\n",
    "        qa.retriever.search_kwargs.update({\"k\": k_slider.value})\n",
    "    except Exception as e:\n",
    "        _append(f\"[Aviso] Não consegui ajustar k: {e}\")\n",
    "\n",
    "k_slider.observe(_set_k, names='value')\n",
    "\n",
    "def _append(text: str):\n",
    "    # adiciona linha e força scroll ao final\n",
    "    log.value = (log.value + (\"\" if log.value.endswith(\"\\n\") else \"\\n\") + text).rstrip() + \"\\n\"\n",
    "\n",
    "busy = False  # trava anti-duplicidade\n",
    "\n",
    "def _ask(q: str):\n",
    "    global busy\n",
    "    q = q.strip()\n",
    "    if not q or busy:\n",
    "        return\n",
    "    busy = True\n",
    "    try:\n",
    "        _append(f\"Você: {q}\")\n",
    "        resp = qa.invoke({\"query\": q})\n",
    "        answer = (resp.get(\"result\") or \"\").strip()\n",
    "        _append(f\"Assistente: {answer}\\n\")\n",
    "        if show_sources.value:\n",
    "            fontes = resp.get(\"source_documents\", []) or []\n",
    "            if fontes:\n",
    "                _append(\"Fontes:\")\n",
    "                for d in fontes:\n",
    "                    src = d.metadata.get(\"source\", \"desconhecida\")\n",
    "                    snippet = shorten((d.page_content or \"\").replace(\"\\n\", \" \"), width=140, placeholder=\"...\")\n",
    "                    _append(f\" - {src}: {snippet}\")\n",
    "                _append(\"\")  # linha em branco\n",
    "    except Exception as e:\n",
    "        _append(f\"[Erro] {e}\")\n",
    "    finally:\n",
    "        busy = False\n",
    "\n",
    "def _submit(_):\n",
    "    q = inp.value\n",
    "    inp.value = \"\"\n",
    "    _ask(q)\n",
    "\n",
    "def _click(_):\n",
    "    _submit(None)\n",
    "\n",
    "def _clear(_):\n",
    "    log.value = \"\"\n",
    "\n",
    "inp.on_submit(_submit)\n",
    "send_btn.on_click(_click)\n",
    "clear_btn.on_click(_clear)\n",
    "\n",
    "ui = VBox([\n",
    "    HBox([inp, send_btn, clear_btn]),\n",
    "    HBox([k_slider, show_sources]),\n",
    "    log\n",
    "])\n",
    "display(ui)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8bfbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
